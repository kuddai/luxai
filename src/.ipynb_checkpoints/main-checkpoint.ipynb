{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import collections\n",
    "import datetime\n",
    "import gc\n",
    "import json\n",
    "import os\n",
    "import pickle\n",
    "import threading\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "from tqdm.notebook import trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "for gpu in tf.config.list_physical_devices('GPU'):\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import solution.constants as const\n",
    "import solution.model as model\n",
    "import solution.utils as utils\n",
    "from solution.replay_buffer import ReplayBuffer\n",
    "from solution.rl import n_step_return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_replay_data(delete_processed_files=False):\n",
    "    with tf.device('CPU:0'):\n",
    "        current_chunk = []\n",
    "        for file_name in os.listdir(const.REPLAY_PATH):\n",
    "            file_path = os.path.join(const.REPLAY_PATH, file_name)\n",
    "            with open(file_path, 'rb') as f:\n",
    "                data = pickle.load(f)\n",
    "                current_chunk += data\n",
    "            \n",
    "            if delete_processed_files:\n",
    "                os.unlink(file_path)\n",
    "    return current_chunk\n",
    "\n",
    "\n",
    "MAX_REPLAY_BUFFER_SIZE = 5000\n",
    "replay_buffer = ReplayBuffer(MAX_REPLAY_BUFFER_SIZE)\n",
    "\n",
    "\n",
    "def replay_buffer_job():\n",
    "    clear_dir(const.REPLAY_PATH)\n",
    "    step = 0\n",
    "    total_sequences = 0\n",
    "    \n",
    "    while replay_data_needed:\n",
    "        chunk = get_replay_data(True)\n",
    "        if not chunk:\n",
    "            time.sleep(1.0)\n",
    "            continue\n",
    "            \n",
    "        if step == 0:\n",
    "            start_time = time.time()\n",
    "            \n",
    "        total_sequences += len(chunk)\n",
    "        \n",
    "        with tf.device('CPU:0'):\n",
    "            add_start_time = time.time()\n",
    "            replay_buffer.add(chunk)\n",
    "            with tf_writer_lock, tf_writer.as_default(), tf.name_scope('replay_buffer'):\n",
    "                tf.summary.scalar('buffer size', len(replay_buffer), step)\n",
    "                tf.summary.scalar('add chunk time', time.time() - add_start_time, step)\n",
    "                tf.summary.scalar('sequences per second', total_sequences / (time.time() - start_time), step)\n",
    "\n",
    "        step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def games_data_job():\n",
    "    clear_dir(const.GAMES_DATA_PATH)\n",
    "    step = collections.defaultdict(int)\n",
    "    \n",
    "    while games_data_needed:\n",
    "        for file_name in os.listdir(const.GAMES_DATA_PATH):\n",
    "            file_path = os.path.join(const.GAMES_DATA_PATH, file_name)\n",
    "            with open(file_path) as f:\n",
    "                game_data = json.load(f)\n",
    "                agent_id = game_data['agent_id']\n",
    "                \n",
    "                for k, v in game_data.items():\n",
    "                    if k == 'agent_id':\n",
    "                        continue\n",
    "                    \n",
    "                    if v is None:\n",
    "                        continue\n",
    "                        \n",
    "                    with tf_writer_lock, tf_writer.as_default():\n",
    "                        with tf.name_scope('game_data/total'):\n",
    "                            tf.summary.scalar(k, v, step['total'])\n",
    "                            \n",
    "                        with tf.name_scope('game_data/%s' % agent_id):\n",
    "                            tf.summary.scalar(k, v, step[agent_id])\n",
    "                    \n",
    "                    \n",
    "                step['total'] += 1\n",
    "                step[agent_id] += 1\n",
    "\n",
    "            os.unlink(file_path)\n",
    "\n",
    "        time.sleep(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soft_update(source_variables, target_variables, tau):\n",
    "    for v_s, v_t in zip(source_variables, target_variables):\n",
    "        v_t.assign((1 - tau) * v_t + tau * v_s)\n",
    "\n",
    "\n",
    "\n",
    "def save_model(name, epoch, model, link_path=None):\n",
    "    if not os.path.exists(const.MODELS_PATH):\n",
    "        os.makedirs(const.MODELS_PATH)\n",
    "        \n",
    "    file_path = os.path.join(const.MODELS_PATH, '%s_%s_%s' % (name, current_time, epoch))\n",
    "    model.save_weights(file_path)\n",
    "    \n",
    "    if link_path is not None:\n",
    "        tmp_link_path = '%s_tmp' % link_path\n",
    "        os.symlink(file_path, tmp_link_path)\n",
    "        os.rename(tmp_link_path, link_path)\n",
    "\n",
    "\n",
    "def clear_dir(path):\n",
    "    if not os.path.exists(path):\n",
    "        os.makedirs(path)\n",
    "    \n",
    "    for file_name in os.listdir(path):\n",
    "        file_path = os.path.join(path, file_name)\n",
    "        os.unlink(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.random.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "sequential (Sequential)      (64, 256)                 215176    \n",
      "_________________________________________________________________\n",
      "sequential_1 (Sequential)    (64, 64)                  11584     \n",
      "_________________________________________________________________\n",
      "sequential_2 (Sequential)    (2048, 256)               777456    \n",
      "_________________________________________________________________\n",
      "sequential_3 (Sequential)    (64, 32, 32)              1312      \n",
      "_________________________________________________________________\n",
      "sequential_4 (Sequential)    (64, 32, 256)             205568    \n",
      "_________________________________________________________________\n",
      "sequential_5 (Sequential)    (64, 32, 512)             558080    \n",
      "_________________________________________________________________\n",
      "sequential_6 (Sequential)    (64, 32, 7)               133127    \n",
      "_________________________________________________________________\n",
      "sequential_7 (Sequential)    (64, 32, 1)               131585    \n",
      "=================================================================\n",
      "Total params: 2,033,888\n",
      "Trainable params: 2,033,888\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE = 64\n",
    "input_shape = [(BATCH_SIZE,) + tuple(i.shape) for i in utils.STATE_SPEC]\n",
    "\n",
    "network = model.Model()\n",
    "network.build(input_shape)\n",
    "network.summary()\n",
    "#latest_path = tf.train.latest_checkpoint(const.MODELS_PATH)\n",
    "#print(latest_path)\n",
    "#network.load_weights(latest_path)\n",
    "\n",
    "target_network = model.Model()\n",
    "target_network.build(input_shape)\n",
    "target_network.trainable = False\n",
    "target_network.set_weights(network.get_weights())\n",
    "\n",
    "\n",
    "optimizer = keras.optimizers.Adam(learning_rate=5e-4, clipnorm=40)\n",
    "loss_fn = keras.losses.MeanSquaredError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def learn_from_batch(states, actions, rewards, is_not_done, *, gamma, weights, online_network, target_network):\n",
    "    #print('Tracing lfb')\n",
    "    # states is tuple of 5 state_element, state_element has (B, N, *) shape\n",
    "    target = n_step_return(\n",
    "        states, actions, rewards, is_not_done,\n",
    "        gamma=gamma, n=const.N_STEPS, online_network=online_network, target_network=target_network\n",
    "    )\n",
    "        \n",
    "    first_state = tuple([state[:, 0] for state in states])\n",
    "    first_action = actions[:, 0]\n",
    "    first_units_mask = tf.cast(states[4][:, 0], dtype=tf.float32)\n",
    "    \n",
    "    with tf.GradientTape() as tape:\n",
    "        q_values = online_network(first_state, training=True)\n",
    "        prediction = tf.gather(q_values, first_action, axis=-1, batch_dims=2)\n",
    "        prediction = tf.reduce_sum(prediction * first_units_mask, axis=-1)\n",
    "        \n",
    "        td_error = target - prediction\n",
    "        #loss = tf.reduce_mean(tf.math.pow(td_error, 2))\n",
    "        loss = tf.reduce_mean(tf.math.pow(td_error, 2) * weights)\n",
    "    \n",
    "    gradients = tape.gradient(loss, online_network.trainable_weights)\n",
    "    grad_min = tf.reduce_mean([tf.reduce_min(g) for g in gradients])\n",
    "    grad_max = tf.reduce_max([tf.reduce_max(g) for g in gradients])\n",
    "    grad_l2 = tf.norm([tf.norm(g) for g in gradients])\n",
    "    optimizer.apply_gradients(zip(gradients, online_network.trainable_weights))\n",
    "    \n",
    "    return loss, tf.math.abs(td_error), grad_min, grad_max, grad_l2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "PermissionDeniedError",
     "evalue": "/tmp/tb_logs/20211001-120436; Permission denied [Op:CreateSummaryFileWriter]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mPermissionDeniedError\u001b[0m                     Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-bc17eda57f38>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mcurrent_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%Y%m%d-%H%M%S\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0mcurrent_logs_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mTB_LOGS_DIR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_time\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mtf_writer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcreate_file_writer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_logs_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0mtf_writer_lock\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mLock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py\u001b[0m in \u001b[0;36mcreate_file_writer_v2\u001b[0;34m(logdir, max_queue, flush_millis, filename_suffix, name, experimental_trackable)\u001b[0m\n\u001b[1;32m    552\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m         return _ResourceSummaryWriter(\n\u001b[0;32m--> 554\u001b[0;31m             create_fn=create_fn, init_op_fn=init_op_fn)\n\u001b[0m\u001b[1;32m    555\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    556\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/summary_ops_v2.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, create_fn, init_op_fn)\u001b[0m\n\u001b[1;32m    311\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_fn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minit_op_fn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resource\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_init_op\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minit_op_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_resource\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_closed\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/ops/gen_summary_ops.py\u001b[0m in \u001b[0;36mcreate_summary_file_writer\u001b[0;34m(writer, logdir, max_queue, flush_millis, filename_suffix, name)\u001b[0m\n\u001b[1;32m    144\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    145\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 146\u001b[0;31m       \u001b[0m_ops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from_not_ok_status\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    147\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    148\u001b[0m       \u001b[0;32mpass\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/tensorflow/python/framework/ops.py\u001b[0m in \u001b[0;36mraise_from_not_ok_status\u001b[0;34m(e, name)\u001b[0m\n\u001b[1;32m   6939\u001b[0m   \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\" name: \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6940\u001b[0m   \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 6941\u001b[0;31m   \u001b[0msix\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mraise_from\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_status_to_exception\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcode\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   6942\u001b[0m   \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   6943\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/six.py\u001b[0m in \u001b[0;36mraise_from\u001b[0;34m(value, from_value)\u001b[0m\n",
      "\u001b[0;31mPermissionDeniedError\u001b[0m: /tmp/tb_logs/20211001-120436; Permission denied [Op:CreateSummaryFileWriter]"
     ]
    }
   ],
   "source": [
    "# Setup tensorboard\n",
    "TB_LOGS_DIR = '/tmp/tb_logs/'\n",
    "\n",
    "if not os.path.exists(TB_LOGS_DIR):\n",
    "    os.mkdir(TB_LOGS_DIR)\n",
    "\n",
    "current_time = datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
    "current_logs_dir = os.path.join(TB_LOGS_DIR, current_time)\n",
    "tf_writer = tf.summary.create_file_writer(current_logs_dir)\n",
    "tf_writer_lock = threading.Lock()\n",
    "\n",
    "steps = 0\n",
    "\n",
    "save_model('online', 0, network, const.LATEST_MODEL_SYMLINK_PATH)\n",
    "save_model('target', 0, target_network, const.TARGET_MODEL_SYMLINK_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def stop_threads():\n",
    "    if 'replay_buffer_thread' in globals():\n",
    "        print('Stopping replay thread')\n",
    "        global replay_data_needed\n",
    "        replay_data_needed = False\n",
    "        if replay_buffer_thread.is_alive():\n",
    "            replay_buffer_thread.join()\n",
    "    \n",
    "    if 'games_data_thread' in globals():\n",
    "        print('Stopping games data thread')\n",
    "        global games_data_needed\n",
    "        games_data_needed = False\n",
    "        if games_data_thread.is_alive():\n",
    "            games_data_thread.join()\n",
    "\n",
    "\n",
    "stop_threads()\n",
    "\n",
    "\n",
    "replay_data_needed = True\n",
    "replay_buffer_thread = threading.Thread(target=replay_buffer_job)\n",
    "replay_buffer_thread.start()\n",
    "\n",
    "games_data_needed = True\n",
    "games_data_thread = threading.Thread(target=games_data_job)\n",
    "games_data_thread.start()\n",
    "\n",
    "\n",
    "EPOCHS = 10000000\n",
    "TARGET_NETWORK_UPDATE_EPISODES = 1000\n",
    "MODEL_SAVE_PERIOD = 100\n",
    "INITIAL_DATA_SIZE = BATCH_SIZE * 10\n",
    "\n",
    "\n",
    "while len(replay_buffer) < INITIAL_DATA_SIZE:\n",
    "    print('Waiting for data: %s / %s' % (len(replay_buffer), INITIAL_DATA_SIZE))\n",
    "    time.sleep(1)\n",
    "\n",
    "        \n",
    "for epoch in trange(EPOCHS):\n",
    "    with tf.device('CPU:0'):\n",
    "        start_time = time.time()\n",
    "        ts, ind, (batch_states, batch_actions, batch_rewards, batch_is_not_done), td, weights = replay_buffer.get_prioritized(BATCH_SIZE)\n",
    "        #(batch_states, batch_actions, batch_rewards, batch_is_not_done), td = replay_buffer.get_uniform(BATCH_SIZE)\n",
    "        get_time = time.time() - start_time\n",
    "    \n",
    "    if steps % TARGET_NETWORK_UPDATE_EPISODES == 0:\n",
    "        print(epoch, 'Updating network')\n",
    "        target_network.set_weights(network.get_weights())\n",
    "        save_model('target', epoch, target_network, const.TARGET_MODEL_SYMLINK_PATH)\n",
    "        gc.collect()\n",
    "        tf.keras.backend.clear_session()\n",
    "        # soft_update(network.variables, target_network.variables, 0.01)\n",
    "    \n",
    "    if steps % MODEL_SAVE_PERIOD == 0:\n",
    "        print(epoch, 'Saving model')\n",
    "        save_model('online', epoch, network, const.LATEST_MODEL_SYMLINK_PATH)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    #loss, new_td_error, grad_min, grad_max, grad_l2 = learn_from_batch(\n",
    "    #    batch_states, batch_actions, batch_rewards, batch_is_not_done,\n",
    "    #    gamma=const.GAMMA, weights=None, online_network=network, target_network=target_network)\n",
    "    loss, new_td_error, grad_min, grad_max, grad_l2 = learn_from_batch(\n",
    "        batch_states, batch_actions, batch_rewards, batch_is_not_done,\n",
    "        gamma=const.GAMMA, weights=weights, online_network=network, target_network=target_network)\n",
    "    learn_time = time.time() - start_time\n",
    "    \n",
    "    start_time = time.time()\n",
    "    replay_buffer.update(ts, ind, new_td_error)\n",
    "    update_time = time.time() - start_time\n",
    "\n",
    "    with tf_writer_lock, tf_writer.as_default():\n",
    "        tf.summary.scalar('loss', loss, step=steps)\n",
    "        #tf.summary.scalar('avg_version', avg_model_version, step=steps)\n",
    "        with tf.name_scope('td_error'):\n",
    "            tf.summary.scalar('before', tf.reduce_mean(td), step=steps)\n",
    "            tf.summary.scalar('after', tf.reduce_mean(new_td_error), step=steps)\n",
    "        with tf.name_scope('timing'):\n",
    "            tf.summary.scalar('get', get_time, step=steps)\n",
    "            tf.summary.scalar('update', update_time, step=steps)\n",
    "            tf.summary.scalar('learn', learn_time, step=steps)\n",
    "        with tf.name_scope('gradients'):\n",
    "            tf.summary.scalar('min', grad_min, step=steps)\n",
    "            tf.summary.scalar('max', grad_max, step=steps)\n",
    "            tf.summary.scalar('l2 norm', grad_l2, step=steps)\n",
    "            \n",
    "    steps += 1\n",
    "\n",
    "stop_threads()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_threads()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
